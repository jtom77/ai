\documentclass[a4paper,10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{luamplib}
% \everymplib{input mpcolornames; beginfig(1);}
% \everyendmplib{endfig;}
\usepackage{geometry}
\geometry{
    a4paper,
    left=20mm,
    top=20mm,
}

\usepackage[T1]{fontenc}
% \usepackage{tgbonum}

\usepackage{fouriernc}

% \usepackage{fontspec}
% \usepackage{unicode-math}
% \setmainfont{TeX Gyre Bonum}
% \setmathfont[math-style=ISO,bold-style=ISO]{TeX Gyre Bonum Math}

\author{Thomas Epp}
\newcommand{\vc}[1]{\boldsymbol{#1}}
\setlength{\parskip}{0.5em} \setlength{\parindent}{0pt} \title{Backpropagation in Neural Networks} \date{2023\\ December}

\def\Loss{\mathcal{L}}

\begin{document}

\maketitle

\section{Backprop for fully connected networks}

\def\inn{\text{in}}

\subsection{Notations}

\begin{center}
    \includegraphics{backprop-1.pdf}
\end{center}

\begin{itemize}
    \item $w_{jk}^\ell$: Weight connecting cell $k$ of layer $\ell -1$ to cell $j$ of layer $\ell$.
    \item $b_j^\ell$: Bias of cell $j$ in layer $\ell$
    \item $z_j^\ell$. Input of cell $j$ in layer $\ell$, $z_j^\ell = \sum_j w_{jk}^\ell a_k^{\ell-1} + b_j^\ell$
    \item $a_j^\ell$: Activation of cell $j$ in layer $\ell$, $a_k^{\ell} = g(z_k^\ell)$, where $g$ is the activation function.
    \item $y_j$: Label at output layer $L$
    \item $C=\frac12 \| \vc{y} - \vc{a}^L \|^2 = \frac12 \sum_{k} | y_k - a_k^L |^2$. Error function
          It is $\vc{z}^{\ell} = W\vc{a}^{\ell-1} + \vc{b}^\ell$
\end{itemize}

We further use the vectorized versions $\vc{a}^\ell$, $\vc{b}^\ell$,
$\vc{z}^\ell$ and $\vc{y}$ and the weight matrix from layer $\ell-1$ to layer
$\ell$:
\[
    W^\ell =
    \begin{pmatrix}
        w_{00}^\ell & w_{01}^\ell & \cdots & w_{0n}^\ell \\
        w_{10}^\ell & w_{11}^k    & \cdots & w_{1n}^\ell \\
        \vdots      & \vdots      & \ddots & \vdots      \\
        w_{m0}^\ell & w_{m1}^\ell & \cdots & w_{mn}^\ell \\
    \end{pmatrix}
\]
The inputs to layer $\ell$ can then be computed by $\vc{z}^\ell =
    W\vc{z}^{\ell-1}+\vc{b}^\ell$

\subsection{The problem}

The aim of the backpropagation is to compute the gradient of the error function
with respect to all the weights $w_{jk}^\ell$ and biases $b_j^\ell$.
\[
    \nabla C =
    \left(\frac{\partial C}{\partial w_0}, \ldots,\frac{\partial C}{\partial w_N}, \ldots,\frac{\partial C}{\partial b_0}, \ldots,\frac{\partial C}{\partial b_M} \right)^T,
\]
where $N$ and $M$ define the total numbers of weights and biases.

\subsection{Expressing as $\partial C / \partial W^\ell$ as a function of the inputs and the output gradient}


Note that
\[
    \frac{\partial \Loss}{\partial w_{jk}^\ell} 
    = \frac{\partial \Loss}{\partial z_j^\ell} \frac{\partial z_j^\ell}{\partial w_{jk}^\ell} 
    = \frac{\partial \Loss}{\partial z_j^\ell} \left[ \frac{\partial}{\partial w_{jk}^\ell} \sum_i w_{ji}^\ell a_{i}^{\ell-1} + b_j^\ell \right] 
    = \frac{\partial \Loss}{\partial z_j^\ell} a_{k}^{\ell-1}.
\]
Substituting $b_j^\ell$ for $\partial w_{jk}^\ell$ above gives a similar result
\[
    \frac{\partial \Loss}{\partial b_{j}^\ell} 
    = \sum_i \frac{\partial \Loss}{\partial z_i^\ell} \frac{\partial z_i^\ell}{\partial b_j^\ell} = 
    \frac{\partial \Loss}{\partial z_{j}^\ell} 
\]

\begin{equation}
    \boxed{
        \frac{\partial \Loss}{\partial W^\ell} = \frac{\partial\Loss}{\partial Z^\ell} \otimes A^{\ell -1}, \qquad
        \frac{\partial \Loss}{\partial B^\ell} = \frac{\partial\Loss}{\partial Z^\ell}
    }
\end{equation}
    


% Note that
% \[
%     \frac{\partial C}{\partial w_{jk}^\ell} 
%     = \frac{\partial C}{\partial z_j^\ell} \frac{\partial z_j^\ell}{\partial w_{jk}^\ell} 
%     = \frac{\partial C}{\partial z_j^\ell} \left[ \frac{\partial}{\partial w_{jk}^\ell} \sum_i w_{ji}^\ell a_{i}^{\ell-1} + b_j^\ell \right] 
%     = \frac{\partial C}{\partial z_j^\ell} a_{k}^{\ell-1}.
% \]
% Substituting $b_j^\ell$ for $\partial w_{jk}^\ell$ above gives a similar result
% \[
%     \frac{\partial C}{\partial b_{j}^\ell} = \frac{\partial C}{\partial z_k^\ell}
% \]

\subsection{Propagate the the gradient of the outputs $z^\ell$ to the previous layer}

% Therefore, the components of the gradient can be computed by the partial
% derivative
% \[
%     \Delta_j^\ell := \frac{\partial C}{\partial z_j^\ell}.
% \]
The values $\delta_k^\ell$ can be computed from those of the higher layers.
This gives a recursive procedure, starting from the output layer. On the output
layer $L$, we can compute the value directly:
% \[
%     \Delta_j^L = \frac{\partial}{\partial z_j^\ell} \left[\frac12 \left(y_j - g(z_j^L)\right)^2 \right] = -g'(z_j^L)
%     \cdot (y_j - a_j)
% \]

\[
    \frac{\partial \Loss}{\partial a_j^L} = - (y_j - a_j^L).
\]

For a hidden layer $\ell$ we get:
% \begin{align*}
%     \Delta_j^\ell
%      & = \frac{\partial C}{\partial z_j^\ell} = \sum_i \frac{\partial C}{\partial z_i^{\ell+1}} \frac{\partial z_i^{\ell+1}}{\partial z_j^\ell} \\  
%      & = \sum_{i} \frac{\partial C}{\partial z_i^{\ell+1}} \left[ \frac{\partial}{\partial z_j^\ell} \left(\sum_\mu w_{i \mu}^{\ell+1} g(z_\mu^\ell) + b_i^{\ell+1}\right) \right] \\  
%      & = \sum_{i} \frac{\partial C}{\partial z_i^{\ell+1}} \cdot w_{i j}^{\ell+1} \cdot g'(z_j^\ell) = \sum_{i}
%        \Delta_i^{\ell+1} \cdot w_{i j}^{\ell+1} \cdot g'(z_j^\ell)
% \end{align*}

\begin{align*}
    \frac{\partial \Loss}{\partial a_j^\ell} 
    = \sum_{i} \frac{\partial \Loss}{\partial z_i^{\ell +1}} \frac{\partial z_i^{\ell +1}}{\partial a_j^\ell} 
    = \sum_{i} \frac{\partial \Loss}{\partial z_i^{\ell +1}} \left[  \frac{1}{\partial a_j^\ell} \sum_{\mu} w_{i\mu}^{\ell +1} a_\mu + b_i^{\ell +1} \right] 
    = \sum_{i} \frac{\partial \Loss}{\partial z_i^{\ell +1}} w_{ij}.
\end{align*}

Using the weight matrix we can write this as

\begin{equation}
    \boxed{
    \frac{\partial \Loss}{\partial A^\ell} = (W^{\ell +1})^T \frac{\partial \Loss}{\partial Z^{\ell + 1}}.
    }
\end{equation}

% \begin{equation}
%     \vc\Delta^\ell = \left[ (W^{\ell +1})^T \vc\Delta^{\ell +1} \right] \circledcirc g'(\vc z^\ell),
% \end{equation}
% where the symbol $\circledcirc$ means componentwise multiplication.

\section{Convolutional Neural Networks}

As in fully connected networks, a convolutional neural network computes the
input to the next layer applying weights and biases to the input the previous
layer. The backward pass is the same: One can compute the gradient of the
weights in one layer as a function of the output gradient and the input vector. 

Furthermore, the gradient of the output can be propagated back to compute the
gradient of the input, which can then be used to compute the gradient for the
weights and biases of the previous layer.

\begin{center}
    \includegraphics{conv-backprop-1.pdf}
\end{center}

\subsection*{Notation}

We consider a convolutional layer with stride $1$ and padding $0$ and use the
following notations
\begin{itemize}
    \item $A= (a_{ij})$ is the input matrix of dimension $N^2$.
    \item $K= (k_{ij})$ is the Kernel matrix of dimension $M^2$.
    \item $Z= (z_{ij})$ is the output of the convolution operation with dimension ${(N-M+1)}^2$.
    \item $B= (b_{ij})$ is the Bias of the output. This is a matrix with the same dimension as $Z$, where every entry is the same.
\end{itemize}

\subsection*{Forward pass}

With these notations, the forward operation is given by
\[
    Z^{\ell} = A^{\ell -1} \circledast K^\ell + B^\ell,
\]
where the components of $Z$ are given by

\begin{align}
    \label{eq:conv-1}
    z_{ij}^{\ell} = \sum_{\mu, \nu=0}^{|K|-1} a_{i+\mu, j+\nu}^{\ell-1} k_{\mu \nu}^\ell + b^\ell.
\end{align}

\subsection*{Backpropagation: Goal}

For backprogation, we have to compute the following gradients:

\begin{equation}
    \frac{\partial \Loss}{\partial K} = \left( \frac{\partial \Loss}{\partial k_{ij}} \right), \quad 
    \frac{\partial \Loss}{\partial b}, \quad \frac{\partial \Loss}{\partial A} = \left( \frac{\partial \Loss}{\partial a_{ij}} \right),
\end{equation}

\subsection*{Derivation of $\partial \Loss / \partial K$}

From equation~(\ref{eq:conv-1}), we get

\begin{align*}
    \frac{\partial \Loss}{\partial k_{\mu\nu}^\ell} = \sum_{i,j = 0}^{|Z|-1} 
    \frac{\partial \Loss}{\partial z_{ij}^{\ell}} 
    \frac{\partial{z_{ij}^{\ell}}}{\partial k_{\mu\nu}^\ell} = 
    \sum_{i,j=0}^{|Z|-1} a_{i + \mu, j + \nu}^{\ell -1} \frac{\partial \Loss}{z_{ij}^{\ell}}
\end{align*}

Note that this equation has the same structure as equation~\ref{eq:conv-1} and
can therefore be considered as the convolution

\begin{align*}
    \boxed{    
    \frac{\partial \Loss}{\partial K^\ell} = A^{\ell-1} \circledast \frac{\partial \Loss}{\partial Z^{\ell}}.
    }
\end{align*}

In the same manner, we compute

\begin{align*}
    \frac{\partial \Loss}{\partial b^\ell} = \sum_{i,j=0}^{|Z|-1} 
    \frac{\partial \Loss}{\partial z_{ij}^\ell} \frac{\partial z_{ij}^\ell}{\partial b^\ell} 
    = \sum_{i,j=0}^{|Z|-1} \frac{\partial \Loss}{\partial z_{ij}^\ell},
\end{align*}

since $\partial z_{ij} / \partial b = 1$ by equation \ref{eq:conv-1}

\subsection*{Derivation of $\partial \Loss / \partial A$}

\def\ov#1{\langle{#1}\rangle}

Before we proceed, we introduce a convenient way to handle out-of-bounds indices in summations. 

For a $N \times N$ matrix $A = (a_{ij})$ we define
\[
   \ov a_{ij} = \left\{ 
        \begin{array}{ll} 
            a_{ij} &\text{ for }0 \le i,j < N \\
            0      &\text{ otherwise}.
        \end{array}
        \right.   
\]
 
With this notation, we obtain from~(\ref{eq:conv-1}), by substituting $m=i +\nu$ and $n = j + \mu$: 

\begin{equation}
    \label{eq:conv-3}
    \boxed{
        \frac{\partial \Loss}{\partial a^{\ell-1}_{mn}} = 
        \sum_{i,j =0 }^{|Z|-1} \frac{\partial \Loss}{\partial z_{ij}^\ell} 
        \frac{\partial z_{ij}^\ell}{\partial a_{mn}^{\ell-1}} = 
        \sum_{i,j = 0}^{|Z|-1} \frac{\partial \Loss}{\partial z_{ij}^\ell} \ov{k}_{m-i, n-j}^{\ell}.
        }
\end{equation}

\subsubsection*{Convolutional Representation}

\def\flip#1{\widehat{#1}}
\def\pad#1{\widetilde{#1}}


With equation~(\ref{eq:conv-3}), we are already able to compute all the
components of the gradient $\partial \Loss / \partial A$ efficiently.

However, this formula can also be presented as a convolution. The derivation is
merely an exercise in index-transformation.

We denote by $\flip K^\ell$ the ``flipped'' matrix of $K^\ell$ so that 
\[
    \ov{\flip{k}}^\ell_{ij} = \ov{k}^\ell_{|K|-1-i, |K|-1-j}
\]
and by $\pad Z^\ell $ the matrix which is obtained from $Z^\ell$ by adding
a padding of $|K|-1$, so that
\[
    \ov{z}_{ij}^\ell = \ov{\pad{z}}^\ell_{|K|-1+i, |K|-1+j}
\]

With these notations, we obtain from~(\ref{eq:conv-3}):

\begin{align*}
    \frac{\partial \Loss}{\partial a_{mn}^{\ell-1}} &= 
    \sum_{i,j=0}^{|Z|-1}
    \frac{\partial \Loss}{\partial \pad z_{|K|-1+i,|K|-1+j}^\ell} \ov{k}_{m-i, n-j}^\ell 
    = \sum_{i,j=0}^{|Z|-1}
    \frac{\partial \Loss}{\partial \pad z_{|K|-1+i, |K|-1+j}^\ell}
    \ov{\flip k}^\ell_{|K|-1+i-m, |K|-1+j-n} \\
    \intertext{Substituting $\mu:  i\mapsto |K|-1+i-m$, $\nu: j \mapsto |K|-1+j-n$} 
    \frac{\partial \Loss}{\partial a_{mn}^{\ell-1}}
    &= \sum_{\mu = |K|-1-m}^{|A|-m} \sum_{\nu = |K|-1-n}^{|A|-n} 
    \frac{\partial \Loss}{\partial \pad z_{\mu + m, \nu + n} ^\ell} \ov{\flip k}_{\nu \mu} = 
    \sum_{\mu, \nu = 0}^{|K|-1} 
    \frac{\partial \Loss}{\partial \pad z_{\mu + m, \nu + n}^\ell} \flip k_{\nu \mu}^\ell,
\end{align*}

where the last equation follow from the fact that
$\ov{\flip k}^\ell_{\nu \mu} $ is only non-zero for $0\le \mu, \nu < |K|$ and
$\partial \Loss / \partial \pad z^\ell_{ij} =0$ for $i \ge |Z|$ or $j \ge |Z|$.

The last equation has the general form of a convolution, and so we get

\begin{equation}
    \boxed{
        \frac{\partial \Loss}{\partial A^{\ell-1}} = \frac{\partial \Loss}{\partial \pad Z^\ell} 
        \circledast \flip K_{\nu \mu}^\ell }.
\end{equation}

\end{document}
