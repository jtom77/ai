\documentclass[a4paper,10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\vc}[1]{\boldsymbol{#1}}
\setlength{\parskip}{0.5em} \setlength{\parindent}{0pt}
\title{Backpropagation in Neural Networks}
\date{2023\\ December}
\begin{document}

\maketitle

\section{Backprop for fully connected networks}

\def\inn{\text{in}}

\subsection{Notations}

\includegraphics{backprop-1.pdf}

\begin{itemize}
    \item $w_{jk}^\ell$: Weight connecting cell $k$ of layer $\ell -1$ to cell $j$ of layer $\ell$.
    \item $b_j^\ell$: Bias of cell $j$ in layer $\ell$
    \item $z_j^\ell$. Input of cell $j$ in layer $\ell$, $z_j^\ell = \sum_j w_{jk}^\ell a_k^{\ell-1} + b_j^\ell$
    \item $a_j^\ell$: Activation of cell $j$ in layer $\ell$, $a_k^{\ell} = g(z_k^\ell)$, where $g$ is the activation function.
    \item $y_j$: Label at output layer $L$
    \item $C=\frac12 \| \vc{y} - \vc{a}^L \|^2 = \frac12 \sum_{k} | y_k - a_k^L |$. Error function
          It is $\vc{z}^{\ell} = W\vc{a}^{\ell-1} + \vc{b}^\ell$
\end{itemize}

We further use the vectorized versions $\vc{a}^\ell$, $\vc{b}^\ell$,
$\vc{z}^\ell$ and $\vc{y}$ and the weight matrix from layer $\ell-1$ to layer
$\ell$:
\[
    W^\ell =
    \begin{pmatrix}
        w_{00}^\ell & w_{01}^\ell & \cdots & w_{0n}^\ell \\
        w_{10}^\ell & w_{11}^k    & \cdots & w_{1n}^\ell \\
        \vdots      & \vdots      & \ddots & \vdots      \\
        w_{m0}^\ell & w_{m1}^\ell & \cdots & w_{mn}^\ell \\
    \end{pmatrix}
\]
The inputs to layer $\ell$ can then be computed by $\vc{z}^\ell =
    W\vc{z}^{\ell-1}+\vc{b}^\ell$

\subsection{The problem}

The aim of the backpropagation is to compute the gradient of the error function
with respect to all the weights $w_{jk}^\ell$ and biases $b_j^\ell$.
\[
    \nabla C =
    \left(\frac{\partial C}{\partial w_0}, \ldots,\frac{\partial C}{\partial w_N}, \ldots,\frac{\partial C}{\partial b_0}, \ldots,\frac{\partial C}{\partial b_M} \right)^T,
\]
where $N$ and $M$ define the total numbers of weights and biases.

\subsection{Expressing as $\partial C / \partial W^\ell$ as a function of the inputs and the output gradient}

Note that
\[
    \frac{\partial C}{\partial w_{jk}^\ell} = \frac{\partial C}{\partial z_j^\ell} \frac{\partial z_j^\ell}{\partial w_{jk}^\ell} = \frac{\partial C}{\partial z_j^\ell} \left[ \frac{\partial}{\partial w_{jk}^\ell} \sum_i w_{ji}^\ell a_{i}^{\ell-1} + b_j^\ell \right] = \frac{\partial C}{\partial z_j^\ell} a_{k}^{\ell-1}.
\]
Substituting $b_j^\ell$ for $\partial w_{jk}^\ell$ above gives a similar result
\[
    \frac{\partial C}{\partial b_{j}^\ell} = \frac{\partial C}{\partial z_k^\ell}
\]

\subsection{Propagate the the gradient of the outputs $z^\ell$ to the previous layer}

Therefore, the components of the gradient can be computed by the partial
derivative
\[
    \Delta_j^\ell := \frac{\partial C}{\partial z_j^\ell}.
\]
The values $\delta_k^\ell$ can be computed from those of the higher layers.
This gives a recursive procedure, starting from the output layer. On the output
layer $L$, we can compute the value directly:
\[
    \Delta_j^L = \frac{\partial}{\partial z_j^\ell} \left[\frac12 \left(y_j - g(z_j^L)\right)^2 \right] = -g'(z_j^L)
    \cdot (y_j - a_j)
\]
For a hidden layer $\ell$ we get:
\begin{align*}
    \Delta_j^\ell & = \frac{\partial C}{\partial z_j^\ell} = \sum_i \frac{\partial C}{\partial z_i^{\ell+1}} \frac{\partial z_i^{\ell+1}}{\partial z_j^\ell} \\  & = \sum_{i} \frac{\partial C}{\partial z_i^{\ell+1}} \left[ \frac{\partial}{\partial z_j^\ell} \left(\sum_\mu w_{i \mu}^{\ell+1} g(z_\mu^\ell) +
    b_i^{\ell+1}\right) \right]                                                                                                                              \\  & = \sum_{i} \frac{\partial C}{\partial z_i^{\ell+1}} \left[ \frac{\partial}{\partial z_j^\ell} \left(w_{i j}^{\ell+1} g(z_j^\ell) + b_i^{\ell+1} \right)
    \right]                                                                                                                                                  \\  & = \sum_{i} \frac{\partial C}{\partial z_i^{\ell+1}} \cdot w_{i j}^{\ell+1} \cdot g'(z_j^\ell) = \sum_{i}
       \Delta_i^{\ell+1} \cdot w_{i j}^{\ell+1} \cdot g'(z_j^\ell)
\end{align*}
Using the weight matrix we can write this as

\begin{equation}
    \vc\Delta^\ell = \left[ (W^{\ell +1})^T \vc\Delta^{\ell +1} \right] \circledcirc g'(\vc z^\ell),
\end{equation}
where the symbol $\circledcirc$ means componentwise multiplication.

\section{Convolutional Neural Networks}

As in fully connected networks, a convolutional neural network 
computes the input to the next layer applying weights and biases 
to the input the previous layer. The backward pass is the same: 
One can compute the gradient of the weights in one layer 
as a function of the output gradient and the input vector. 

Furthermore, the gradient of the output can be propagated back to 
compute the gradient of the input, which can then be used to compute the 
gradient for the weights and biases of the previous layer.

\begin{center}
\includegraphics{conv-backprop-1.pdf}
\end{center}

\subsection*{Notation}

We consider a convolutional layer with stride $1$ and padding $0$ and use the following 
notations 
\begin{itemize}
    \item $A= (a_{ij})$ is the input matrix of dimension $N^2$.
    \item $K= (k_{ij})$ is the Kernel matrix of dimension $M^2$.
    \item $Z= (z_{ij})$ is the output of the convolution operation with dimension ${(N-M+1)}^2$.
    \item $B= (b_{ij})$ is the Bias of the output. This is a matrix with the same dimension as $Z$, where every entry is the same.  
\end{itemize}

\subsection*{Forward pass}

With these notations, the forward operation is given by
\[
    Z = A \circledast K + B,
\]
where the components of $Z$ are given by

\begin{align}
    \label{eq:conv-1}
    z_{ij} = \sum_{\nu=0}^{M-1} \sum_{\mu=0}^{M-1} a_{i+\nu, j+\mu} k_{\nu \mu} + b.
\end{align}

\subsection*{Backpropagation: Goal}

For backprogation, we have to compute the following gradients:

\begin{equation}
    \frac{\partial L}{\partial K} = \left( \frac{\partial L}{\partial k_{ij}} \right), 
    \quad
    \frac{\partial L}{\partial b},
    \quad 
    \frac{\partial L}{\partial A} = \left( \frac{\partial L}{\partial a_{ij}} \right),
\end{equation}

\subsection*{Derivation of $\partial L / \partial K$}

From equation~\ref{eq:conv-1}, it follows that $\frac{\partial z_{ij}}{\partial k_{\nu\mu}} = a_{i + \nu, j + \mu}$, and hence

\begin{align*}
    \frac{\partial L}{\partial k_{\nu\mu}} = \sum_{i,j} 
    \frac{\partial L}{z_{ij}} \frac{\partial{z_{ij}}}{\partial k_{\nu\mu}} = \sum_{i,j} 
    a_{i + \nu, j + \mu}
    \frac{\partial L}{z_{ij}}
\end{align*}

Note that this equation has the same structure as equation~\ref{eq:conv-1} and can
therefore be considered as the convolution

\begin{align*}
    \frac{\partial L}{\partial K} = A \circledast \frac{\partial L}{\partial Z}.
\end{align*}

\subsection*{Derivation of $\partial L / \partial b$}

\begin{align*}
    \frac{\partial L}{\partial b} = \sum_{i,j} \frac{\partial L}{\partial z_{ij}} \frac{\partial z_{ij}}{\partial b} = \sum_{i,j} 
    \frac{\partial L}{\partial z_{ij}},
\end{align*}

since $\partial z_{ij} / \partial b = 1$ by equation \ref{eq:conv-1}

\subsection*{Derivation of $\partial L / \partial A$}

Substituting $m=i + \nu$ and $n = j + \mu$ in equation \ref{eq:conv-1}, we get 

\begin{equation}
    \label{eq:conv-2}
    \boxed{
        \frac{\partial L}{\partial a_{mn}} = \sum_{i,j}
        \frac{\partial L}{\partial z_{ij}} \frac{\partial z_{ij}}{\partial a_{mn}}      
        = \sum_{\substack{i,j}}
        \frac{\partial L}{\partial z_{ij}} \overset{\circ}{k}_{m-i, n-j}.
    }
\end{equation}

where $\overset{\circ}{k}_{\mu \nu}$ is equal to 
$k_{\mu \nu}$ for $0 \le \mu, \nu < M = \dim K$ and equal 
to $0$ otherwise.

\subsubsection*{Convolutional Representation}

With equation~\ref{eq:conv-2}, we are already able to compute 
all the components of the gradient $\partial L / \partial A$ efficiently.

However, this formula can also be presented as a convolution. 
The derivation is merely an exercise in index-transformation.   

We denote by $K^*$ the ``rotated'' matrix of $K$ defined by 
$k^*_{ij} = k_{M-1-i, M-1-j}$ and by $Z^*$ the matrix which is obtained from $Z$ by 
adding a padding of $M-1$, so that 
\[
    z_{ij} = z^*_{i+M-1, j+M-1}
\]

\begin{align*}
    \frac{\partial L}{\partial a_{mn}} &= 
    \sum_{i,j < \dim Z}
    \frac{\partial L}{\partial z^*_{M-1+i,M-1+j}} \overset{\circ}{k}_{m-i, n-j} \\
    &= \sum_{i,j < \dim Z}
    \frac{\partial L}{\partial z^*_{M-1+i, M-1+j}}
    \overset{\circ}{k^*}_{M-1+i-m, M-1+j-n} \\
    \intertext{Substituting $\mu:  i\mapsto M-1+i-m$, $\nu: j \mapsto M-1+j-n$} 
    &= \sum_{\mu = M-1-m}^{N-m} \sum_{\nu = M-1-n}^{N-n} 
    \frac{\partial L}{\partial z^*_{\mu + m, \nu + n}} \overset{\circ}{k^*}_{\nu \mu} = 
    \sum_{\mu = 0}^{M-1}\sum_{\nu = 0}^{M-1} 
    \frac{\partial L}{\partial z^*_{\mu + m, \nu + n}} k^*_{\nu \mu},
\end{align*}
were the last equation follow from the fact that 
$\overset{\circ}{k^*}_{\nu \mu} = 0$ is only non-zero for $0\le \mu, \nu < M$ and 
$\partial L / \partial z^*_{ij} =0$ for $i \ge N$ or $j \ge N$. 

The last equation has the general form of a convolution, and so we get

\begin{equation}
    \boxed{
    \frac{\partial L}{\partial A} = \frac{\partial L}{\partial Z^*} \circledast K^*_{\nu \mu}
    }
\end{equation}

\end{document}
